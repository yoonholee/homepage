---
layout: about
title: About
permalink: /
# description: <a href="#">Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.jpg

news: false
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true
---

I'm a third-year CS Ph.D. student at Stanford, advised by [Chelsea Finn](https://ai.stanford.edu/~cbfinn/) and part of the [IRIS lab](https://irislab.stanford.edu/).
I am affiliated with [SAIL](https://ai.stanford.edu/), [CRFM](https://crfm.stanford.edu/), and the [ML Group](http://ml.stanford.edu/).
My research is partly supported by [KFAS](https://eng.kfas.or.kr/theme/kfaschanel/intl_scholarship_5.php).

Previously, as alternative military service for the South Korean army, I worked as a research scientist at [Kakao](https://www.kakaocorp.com/) and [AITRICS](https://www.aitrics.com/), working with [Juho Lee](https://juho-lee.github.io/).
Before that, I completed my master's (CS, advised by [Seungjin Choi](http://mlg.postech.ac.kr/~seungjin)) and undergraduate (math) degrees at [POSTECH](https://www.postech.ac.kr/eng/).

Real-world conditions are **nonstationary** rather than static.
My research interest is in building **reliable machine learning systems** that can navigate and make sound decisions in such perpetually changing environments.
Here are some key questions that guide my research:

- **Better teaching**: Is there a more efficient and robust way to teach machines, beyond passive observation or imitation, so that they more easily "understand" the underlying concepts? What should we do when we want a machine to perform a task that humans cannot do well?
- **Underspecification**: No dataset fully specifies its intended task. How can we make models recognize and represent the multitude of possible realities consistent with given data?
- **Understanding fine-tuning**: How can we better conceptualize fine-tuning as applied in practice? What knowledge is present in foundation models, and what factors influence how much knowledge is preserved during fine-tuning?
- **De-risking errors**: What strategies can we employ to handle the reality of machine learning systems generating potentially erroneous outputs?
<!-- - **Usable information**: How can we formalize and quantify the amount of information in a dataset that is (1) learnable by a neural network and/or is (2) pertinent to a given task? -->

<div class="selected-papers">
<h3>Selected Papers</h3>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2402.03715">
Clarify: Improving Model Robustness with Natural Language Corrections
</a>
<p class="authors"> 
Yoonho Lee, Michelle Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn
</p>
<p class="venue"> 
NeurIPS 2023 workshops XAIA, ICBINB
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2401.10220">
AutoFT: Learning an Objective for Robust Fine-Tuning
</a>
<p class="authors"> 
Caroline Choi*, Yoonho Lee*, Annie S. Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn
</p>
<p class="venue"> 
NeurIPS 2023 workshop DistShift
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2302.05441">
Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features
</a>
<p class="authors"> 
Annie S. Chen*, Yoonho Lee*, Amrith Setlur, Sergey Levine, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2024 (spotlight)
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2301.11305">
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
</a>
<p class="authors"> 
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn
</p>
<p class="venue"> 
ICML 2023 (long oral) 
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2210.11466">
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts
</a>
<p class="authors"> 
Yoonho Lee*, Annie S. Chen*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2202.03418">
Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement
</a>
<p class="authors"> 
Yoonho Lee, Huaxiu Yao, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://proceedings.mlr.press/v97/lee19d/lee19d.pdf">
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
</a>
<p class="authors"> 
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee Whye Teh
</p>
<p class="venue"> 
ICML 2019
</p>
</div>

</div>