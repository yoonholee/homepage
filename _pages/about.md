---
layout: about
title: About
permalink: /
# description: <a href="#">Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.jpg

news: false
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true
---

I'm a Ph.D. candidate at Stanford CS, advised by [Chelsea Finn](https://ai.stanford.edu/~cbfinn/) and part of the [IRIS lab](https://irislab.stanford.edu/).
I am affiliated with [SAIL](https://ai.stanford.edu/), [CRFM](https://crfm.stanford.edu/), and the [ML Group](http://ml.stanford.edu/) at Stanford.
My research is supported by an [OpenAI Superalignment Fellowship](https://openai.com/index/superalignment-fast-grants/) and a [KFAS PhD Scholarship](https://eng.kfas.or.kr/theme/kfaschanel/intl_scholarship_5.php).

Previously, as mandatory military service for the South Korean army, I was a research scientist at [Kakao](https://www.kakaocorp.com/) and [AITRICS](https://www.aitrics.com/), working with [Juho Lee](https://juho-lee.github.io/).
Before that, I completed my master's (CS, advised by [Seungjin Choi](http://mlg.postech.ac.kr/~seungjin)) and undergraduate (math) degrees at [POSTECH](https://www.postech.ac.kr/eng/).

Here are some key questions that guide my research:

- **Teaching strong models**: Strong pre-trained models already know much of what we want to teach them. Post-training seems to be more about eliciting the appropriate pre-existing capabilities than instilling entirely new information. Can we develop a more effective paradigm for "teaching" models that leverage the pre-existing capabilities inside pre-trained models?
- **Underspecification**: No dataset fully specifies its intended task. How can we make models recognize and represent the multitude of possible realities consistent with given data? What is the best way to leverage such diverse hypotheses?
- **Understanding information**: Within any data, there is an underlying essence ("information") that exists independently of the specific representation. How can we better conceptualize this notion of information, and understand the mechanisms by which machine learning models extract, store, and communicate it?
- **Mitigating risks**: What strategies can we employ to handle the reality of machine learning systems generating potentially erroneous or harmful outputs?


<div class="selected-papers">
<h3>Selected Papers</h3>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2402.03715">
Clarify: Improving Model Robustness with Natural Language Corrections
</a>
<p class="authors"> 
Yoonho Lee, Michelle Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn
</p>
<p class="venue"> 
NeurIPS 2023 workshops XAIA, ICBINB
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2401.10220">
AutoFT: Learning an Objective for Robust Fine-Tuning
</a>
<p class="authors"> 
Caroline Choi*, Yoonho Lee*, Annie S. Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn
</p>
<p class="venue"> 
NeurIPS 2023 workshop DistShift
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2302.05441">
Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features
</a>
<p class="authors"> 
Annie S. Chen*, Yoonho Lee*, Amrith Setlur, Sergey Levine, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2024 (spotlight)
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2301.11305">
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
</a>
<p class="authors"> 
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn
</p>
<p class="venue"> 
ICML 2023 (long oral) 
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2210.11466">
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts
</a>
<p class="authors"> 
Yoonho Lee*, Annie S. Chen*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2202.03418">
Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement
</a>
<p class="authors"> 
Yoonho Lee, Huaxiu Yao, Chelsea Finn
</p>
<p class="venue"> 
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://proceedings.mlr.press/v97/lee19d/lee19d.pdf">
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
</a>
<p class="authors"> 
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee Whye Teh
</p>
<p class="venue"> 
ICML 2019
</p>
</div>

</div>